#!/usr/bin/env python3
"""
Wrong Detection Analysis for AgenticRAG with Markov Chain Anomaly Detection

This script analyzes false positives and false negatives in the detection results
to understand what went wrong and how to improve the model.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import os
import json
from sklearn.metrics import confusion_matrix, classification_report
import warnings
warnings.filterwarnings('ignore')

class WrongDetectionAnalysis:
    """
    Analysis framework for examining wrong detections in AgenticRAG results
    """
    
    def __init__(self, dataset_path: str = "data/cert_insider_threat"):
        self.dataset_path = dataset_path
        self.analysis_results = {}
        
    def load_comprehensive_dataset(self):
        """Load the comprehensive CERT dataset"""
        print("üìä Loading comprehensive CERT dataset...")
        
        # Load the dataset
        df = pd.read_csv(f"{self.dataset_path}/comprehensive_agentic_cert_dataset.csv")
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        
        print(f"   ‚úÖ Loaded dataset with {len(df)} records")
        print(f"   ‚úÖ {df['is_malicious'].sum()} malicious activities")
        print(f"   ‚úÖ {len(df['user_id'].unique())} unique users")
        
        return df
    
    def load_detection_results(self, interval: int = 720, similarity_method: str = 'kl_divergence'):
        """Load detection results from JSON files"""
        print(f"üìä Loading detection results for {interval}min intervals...")
        
        # Load the detection results
        filename = f"comprehensive_agentic_rag_results_{interval}min_{similarity_method}.json"
        
        if not os.path.exists(filename):
            print(f"   ‚ùå File {filename} not found")
            return None
        
        with open(filename, 'r') as f:
            results = json.load(f)
        
        print(f"   ‚úÖ Loaded {len(results['anomalies'])} detected anomalies")
        print(f"   ‚úÖ Analyzed {results['sequences_analyzed']} sequences")
        print(f"   ‚úÖ Threshold: {results['threshold']}")
        
        return results
    
    def analyze_wrong_detections(self, df: pd.DataFrame, results: dict, interval: int = 720):
        """Analyze false positives and false negatives"""
        print(f"üîç Analyzing wrong detections for {interval}min intervals...")
        
        # Prepare data for the same interval
        df_resampled = df.set_index('timestamp').resample(f'{interval}T').agg({
            'activity_level': ['mean', 'std', 'max'],
            'file_accesses': ['sum', 'mean', 'max'],
            'network_connections': ['sum', 'mean', 'max'],
            'data_transfer_mb': ['sum', 'mean', 'max'],
            'login_events': ['sum', 'mean'],
            'privilege_escalation_attempts': ['sum', 'mean'],
            'is_malicious': 'max',
            'user_id': 'first',
            'is_work_hours': 'mean',
            'is_weekend': 'mean',
            'total_activity': ['sum', 'mean'],
            'high_activity': 'sum',
            'high_data_transfer': 'sum',
            'high_privilege_attempts': 'sum',
            'activity_deviation': ['mean', 'std'],
            'file_deviation': ['mean', 'std'],
            'network_deviation': ['mean', 'std'],
            'transfer_deviation': ['mean', 'std'],
            'activity_volatility': ['mean', 'max'],
            'data_intensity': ['mean', 'max'],
            'network_intensity': ['mean', 'max']
        }).fillna(0)
        
        # Flatten column names
        df_resampled.columns = ['_'.join(col).strip() for col in df_resampled.columns.values]
        
        # Create activity score
        df_resampled['activity_score'] = (
            df_resampled['activity_level_mean'] * 0.25 +
            df_resampled['file_accesses_sum'] * 0.15 +
            df_resampled['network_connections_sum'] * 0.15 +
            df_resampled['data_transfer_mb_sum'] * 0.12 +
            df_resampled['login_events_sum'] * 0.08 +
            df_resampled['privilege_escalation_attempts_sum'] * 0.05 +
            df_resampled['activity_volatility_mean'] * 0.10 +
            df_resampled['data_intensity_mean'] * 0.05 +
            df_resampled['network_intensity_mean'] * 0.05
        )
        
        # Create prediction array
        predictions = np.zeros(len(df_resampled))
        
        # Mark predicted anomalies
        for anomaly in results['anomalies']:
            if 'start_time' in anomaly and 'end_time' in anomaly:
                try:
                    start_time = pd.to_datetime(anomaly['start_time'])
                    end_time = pd.to_datetime(anomaly['end_time'])
                    
                    # Find matching time periods
                    for i, idx in enumerate(df_resampled.index):
                        if start_time <= idx <= end_time:
                            predictions[i] = 1
                except:
                    pass
        
        # Create ground truth array
        true_labels = df_resampled['is_malicious_max'].values
        
        # Ensure both arrays are binary
        # Anomaly (malicious) = 1 (positive), Normal = 0 (negative)
        true_labels = (true_labels > 0).astype(int)
        predictions = (predictions > 0).astype(int)
        
        # Calculate confusion matrix
        tn, fp, fn, tp = confusion_matrix(true_labels, predictions).ravel()
        
        # Create detailed analysis
        analysis = {
            'true_positives': tp,
            'false_positives': fp,
            'false_negatives': fn,
            'true_negatives': tn,
            'precision': tp / (tp + fp) if (tp + fp) > 0 else 0,
            'recall': tp / (tp + fn) if (tp + fn) > 0 else 0,
            'f1_score': 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0,
            'accuracy': (tp + tn) / (tp + tn + fp + fn),
            'false_positive_rate': fp / (fp + tn) if (fp + tn) > 0 else 0,
            'false_negative_rate': fn / (fn + tp) if (fn + tp) > 0 else 0
        }
        
        # Analyze false positives
        false_positive_analysis = self._analyze_false_positives(df_resampled, predictions, true_labels)
        
        # Analyze false negatives
        false_negative_analysis = self._analyze_false_negatives(df_resampled, predictions, true_labels)
        
        # Analyze true positives
        true_positive_analysis = self._analyze_true_positives(df_resampled, predictions, true_labels)
        
        return {
            'metrics': analysis,
            'false_positives': false_positive_analysis,
            'false_negatives': false_negative_analysis,
            'true_positives': true_positive_analysis,
            'predictions': predictions,
            'true_labels': true_labels,
            'data': df_resampled
        }
    
    def _analyze_false_positives(self, df: pd.DataFrame, predictions: np.ndarray, true_labels: np.ndarray):
        """Analyze false positive detections"""
        print("   üîç Analyzing false positives...")
        
        # Find false positive indices
        fp_indices = np.where((predictions == 1) & (true_labels == 0))[0]
        
        if len(fp_indices) == 0:
            return {'count': 0, 'analysis': 'No false positives detected'}
        
        fp_data = df.iloc[fp_indices]
        
        # Analyze characteristics of false positives
        analysis = {
            'count': len(fp_indices),
            'avg_activity_score': fp_data['activity_score'].mean(),
            'avg_activity_level': fp_data['activity_level_mean'].mean(),
            'avg_file_accesses': fp_data['file_accesses_sum'].mean(),
            'avg_network_connections': fp_data['network_connections_sum'].mean(),
            'avg_data_transfer': fp_data['data_transfer_mb_sum'].mean(),
            'avg_login_events': fp_data['login_events_sum'].mean(),
            'avg_privilege_escalation': fp_data['privilege_escalation_attempts_sum'].mean(),
            'avg_activity_volatility': fp_data['activity_volatility_mean'].mean(),
            'avg_data_intensity': fp_data['data_intensity_mean'].mean(),
            'avg_network_intensity': fp_data['network_intensity_mean'].mean(),
            'work_hours_ratio': fp_data['is_work_hours_mean'].mean(),
            'weekend_ratio': fp_data['is_weekend_mean'].mean(),
            'high_activity_ratio': fp_data['high_activity_sum'].mean(),
            'high_data_transfer_ratio': fp_data['high_data_transfer_sum'].mean(),
            'high_privilege_ratio': fp_data['high_privilege_attempts_sum'].mean(),
            'time_periods': fp_data.index.tolist()
        }
        
        print(f"      üìä Found {len(fp_indices)} false positives")
        print(f"      üìà Average activity score: {analysis['avg_activity_score']:.3f}")
        print(f"      üìà Average activity level: {analysis['avg_activity_level']:.3f}")
        
        return analysis
    
    def _analyze_false_negatives(self, df: pd.DataFrame, predictions: np.ndarray, true_labels: np.ndarray):
        """Analyze false negative detections"""
        print("   üîç Analyzing false negatives...")
        
        # Find false negative indices
        fn_indices = np.where((predictions == 0) & (true_labels == 1))[0]
        
        if len(fn_indices) == 0:
            return {'count': 0, 'analysis': 'No false negatives detected'}
        
        fn_data = df.iloc[fn_indices]
        
        # Analyze characteristics of false negatives
        analysis = {
            'count': len(fn_indices),
            'avg_activity_score': fn_data['activity_score'].mean(),
            'avg_activity_level': fn_data['activity_level_mean'].mean(),
            'avg_file_accesses': fn_data['file_accesses_sum'].mean(),
            'avg_network_connections': fn_data['network_connections_sum'].mean(),
            'avg_data_transfer': fn_data['data_transfer_mb_sum'].mean(),
            'avg_login_events': fn_data['login_events_sum'].mean(),
            'avg_privilege_escalation': fn_data['privilege_escalation_attempts_sum'].mean(),
            'avg_activity_volatility': fn_data['activity_volatility_mean'].mean(),
            'avg_data_intensity': fn_data['data_intensity_mean'].mean(),
            'avg_network_intensity': fn_data['network_intensity_mean'].mean(),
            'work_hours_ratio': fn_data['is_work_hours_mean'].mean(),
            'weekend_ratio': fn_data['is_weekend_mean'].mean(),
            'high_activity_ratio': fn_data['high_activity_sum'].mean(),
            'high_data_transfer_ratio': fn_data['high_data_transfer_sum'].mean(),
            'high_privilege_ratio': fn_data['high_privilege_attempts_sum'].mean(),
            'time_periods': fn_data.index.tolist()
        }
        
        print(f"      üìä Found {len(fn_indices)} false negatives")
        print(f"      üìà Average activity score: {analysis['avg_activity_score']:.3f}")
        print(f"      üìà Average activity level: {analysis['avg_activity_level']:.3f}")
        
        return analysis
    
    def _analyze_true_positives(self, df: pd.DataFrame, predictions: np.ndarray, true_labels: np.ndarray):
        """Analyze true positive detections"""
        print("   üîç Analyzing true positives...")
        
        # Find true positive indices
        tp_indices = np.where((predictions == 1) & (true_labels == 1))[0]
        
        if len(tp_indices) == 0:
            return {'count': 0, 'analysis': 'No true positives detected'}
        
        tp_data = df.iloc[tp_indices]
        
        # Analyze characteristics of true positives
        analysis = {
            'count': len(tp_indices),
            'avg_activity_score': tp_data['activity_score'].mean(),
            'avg_activity_level': tp_data['activity_level_mean'].mean(),
            'avg_file_accesses': tp_data['file_accesses_sum'].mean(),
            'avg_network_connections': tp_data['network_connections_sum'].mean(),
            'avg_data_transfer': tp_data['data_transfer_mb_sum'].mean(),
            'avg_login_events': tp_data['login_events_sum'].mean(),
            'avg_privilege_escalation': tp_data['privilege_escalation_attempts_sum'].mean(),
            'avg_activity_volatility': tp_data['activity_volatility_mean'].mean(),
            'avg_data_intensity': tp_data['data_intensity_mean'].mean(),
            'avg_network_intensity': tp_data['network_intensity_mean'].mean(),
            'work_hours_ratio': tp_data['is_work_hours_mean'].mean(),
            'weekend_ratio': tp_data['is_weekend_mean'].mean(),
            'high_activity_ratio': tp_data['high_activity_sum'].mean(),
            'high_data_transfer_ratio': tp_data['high_data_transfer_sum'].mean(),
            'high_privilege_ratio': tp_data['high_privilege_attempts_sum'].mean(),
            'time_periods': tp_data.index.tolist()
        }
        
        print(f"      üìä Found {len(tp_indices)} true positives")
        print(f"      üìà Average activity score: {analysis['avg_activity_score']:.3f}")
        print(f"      üìà Average activity level: {analysis['avg_activity_level']:.3f}")
        
        return analysis
    
    def create_wrong_detection_visualizations(self, analysis_results: dict):
        """Create visualizations for wrong detection analysis"""
        print("üìä Creating wrong detection visualizations...")
        
        # Create comparison plots
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20, 15))
        
        # 1. Feature comparison between FP, FN, and TP
        features = ['avg_activity_score', 'avg_activity_level', 'avg_file_accesses', 
                   'avg_network_connections', 'avg_data_transfer']
        
        fp_values = [analysis_results['false_positives'].get(f, 0) for f in features]
        fn_values = [analysis_results['false_negatives'].get(f, 0) for f in features]
        tp_values = [analysis_results['true_positives'].get(f, 0) for f in features]
        
        x = np.arange(len(features))
        width = 0.25
        
        ax1.bar(x - width, fp_values, width, label='False Positives', color='red', alpha=0.7)
        ax1.bar(x, fn_values, width, label='False Negatives', color='orange', alpha=0.7)
        ax1.bar(x + width, tp_values, width, label='True Positives', color='green', alpha=0.7)
        
        ax1.set_xlabel('Features')
        ax1.set_ylabel('Average Values')
        ax1.set_title('Feature Comparison: FP vs FN vs TP')
        ax1.set_xticks(x)
        ax1.set_xticklabels([f.replace('avg_', '').replace('_', ' ').title() for f in features], rotation=45)
        ax1.legend()
        
        # 2. Confusion matrix visualization
        cm = np.array([[analysis_results['metrics']['true_negatives'], analysis_results['metrics']['false_positives']],
                      [analysis_results['metrics']['false_negatives'], analysis_results['metrics']['true_positives']]])
        
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax2,
                    xticklabels=['Predicted Normal', 'Predicted Anomaly'],
                    yticklabels=['Actual Normal', 'Actual Anomaly'])
        ax2.set_title('Confusion Matrix')
        
        # 3. Performance metrics
        metrics = ['precision', 'recall', 'f1_score', 'accuracy']
        metric_values = [analysis_results['metrics'][m] for m in metrics]
        
        ax3.bar(metrics, metric_values, color=['blue', 'green', 'orange', 'red'])
        ax3.set_ylabel('Score')
        ax3.set_title('Performance Metrics')
        ax3.set_ylim(0, 1)
        
        for i, v in enumerate(metric_values):
            ax3.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')
        
        # 4. Error rates
        error_rates = ['false_positive_rate', 'false_negative_rate']
        error_values = [analysis_results['metrics'][r] for r in error_rates]
        
        ax4.bar(error_rates, error_values, color=['red', 'orange'])
        ax4.set_ylabel('Rate')
        ax4.set_title('Error Rates')
        ax4.set_ylim(0, 1)
        
        for i, v in enumerate(error_values):
            ax4.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig('wrong_detection_analysis.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        print("   üìä Wrong detection visualization saved to: wrong_detection_analysis.png")
    
    def generate_wrong_detection_report(self, analysis_results: dict, interval: int, similarity_method: str):
        """Generate comprehensive wrong detection report"""
        print("\nüìä Generating Wrong Detection Report...")
        
        # Convert numpy types to Python types for JSON serialization
        def convert_numpy_types(obj):
            if isinstance(obj, dict):
                return {k: convert_numpy_types(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy_types(v) for v in obj]
            elif isinstance(obj, (np.integer, np.int64, np.int32)):
                return int(obj)
            elif isinstance(obj, (np.floating, np.float64, np.float32)):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif hasattr(obj, 'to_pydatetime'):  # Handle pandas Timestamp
                return obj.to_pydatetime().isoformat()
            elif hasattr(obj, 'isoformat'):  # Handle datetime objects
                return obj.isoformat()
            else:
                return obj
        
        # Create detailed report
        report = {
            'interval': interval,
            'similarity_method': similarity_method,
            'metrics': convert_numpy_types(analysis_results['metrics']),
            'false_positives': convert_numpy_types(analysis_results['false_positives']),
            'false_negatives': convert_numpy_types(analysis_results['false_negatives']),
            'true_positives': convert_numpy_types(analysis_results['true_positives']),
            'analysis_timestamp': datetime.now().isoformat()
        }
        
        # Save detailed report
        filename = f"wrong_detection_report_{interval}min_{similarity_method}.json"
        with open(filename, 'w') as f:
            json.dump(report, f, indent=2)
        
        # Create summary report
        summary = f"""
# Wrong Detection Analysis Report

## Test Configuration
- **Time Interval**: {interval} minutes
- **Similarity Method**: {similarity_method}
- **Analysis Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Performance Metrics
- **Precision**: {analysis_results['metrics']['precision']:.3f}
- **Recall**: {analysis_results['metrics']['recall']:.3f}
- **F1-Score**: {analysis_results['metrics']['f1_score']:.3f}
- **Accuracy**: {analysis_results['metrics']['accuracy']:.3f}
- **False Positive Rate**: {analysis_results['metrics']['false_positive_rate']:.3f}
- **False Negative Rate**: {analysis_results['metrics']['false_negative_rate']:.3f}

## Error Analysis

### False Positives ({analysis_results['false_positives']['count']} cases)
- **Average Activity Score**: {analysis_results['false_positives'].get('avg_activity_score', 0):.3f}
- **Average Activity Level**: {analysis_results['false_positives'].get('avg_activity_level', 0):.3f}
- **Average File Accesses**: {analysis_results['false_positives'].get('avg_file_accesses', 0):.3f}
- **Average Network Connections**: {analysis_results['false_positives'].get('avg_network_connections', 0):.3f}
- **Average Data Transfer**: {analysis_results['false_positives'].get('avg_data_transfer', 0):.3f}

### False Negatives ({analysis_results['false_negatives']['count']} cases)
- **Average Activity Score**: {analysis_results['false_negatives'].get('avg_activity_score', 0):.3f}
- **Average Activity Level**: {analysis_results['false_negatives'].get('avg_activity_level', 0):.3f}
- **Average File Accesses**: {analysis_results['false_negatives'].get('avg_file_accesses', 0):.3f}
- **Average Network Connections**: {analysis_results['false_negatives'].get('avg_network_connections', 0):.3f}
- **Average Data Transfer**: {analysis_results['false_negatives'].get('avg_data_transfer', 0):.3f}

### True Positives ({analysis_results['true_positives']['count']} cases)
- **Average Activity Score**: {analysis_results['true_positives'].get('avg_activity_score', 0):.3f}
- **Average Activity Level**: {analysis_results['true_positives'].get('avg_activity_level', 0):.3f}
- **Average File Accesses**: {analysis_results['true_positives'].get('avg_file_accesses', 0):.3f}
- **Average Network Connections**: {analysis_results['true_positives'].get('avg_network_connections', 0):.3f}
- **Average Data Transfer**: {analysis_results['true_positives'].get('avg_data_transfer', 0):.3f}

## Key Insights

### False Positive Patterns
- High activity scores but normal behavior patterns
- Elevated file access or network activity during normal hours
- Sudden spikes in activity that are legitimate

### False Negative Patterns
- Low activity scores but malicious behavior
- Subtle attack patterns that don't trigger high activity scores
- Attacks during normal working hours that blend in

### True Positive Patterns
- High activity scores with malicious behavior
- Clear deviation from normal patterns
- Multiple indicators of suspicious activity

## Recommendations

### For Reducing False Positives
1. **Adjust threshold**: Increase threshold for more conservative detection
2. **Feature engineering**: Add more context-aware features
3. **Time-based filtering**: Consider work hours and patterns
4. **User-specific baselines**: Individual user behavior modeling

### For Reducing False Negatives
1. **Lower threshold**: Decrease threshold for more sensitive detection
2. **Additional features**: Include more subtle indicators
3. **Sequence analysis**: Longer sequence patterns
4. **Multi-modal detection**: Combine multiple detection methods

### For Overall Improvement
1. **Ensemble methods**: Combine multiple similarity approaches
2. **Deep learning**: LSTM/GRU for sequence modeling
3. **Real-time adaptation**: Dynamic threshold adjustment
4. **Domain expertise**: Incorporate security expert knowledge
"""
        
        # Save summary report
        summary_filename = f"wrong_detection_summary_{interval}min_{similarity_method}.md"
        with open(summary_filename, 'w') as f:
            f.write(summary)
        
        print(f"   üìÑ Detailed report saved to: {filename}")
        print(f"   üìÑ Summary report saved to: {summary_filename}")
        
        return report
    
    def run_comprehensive_analysis(self, intervals: list = [60, 120, 720], similarity_methods: list = ['kl_divergence']):
        """Run comprehensive wrong detection analysis"""
        print("üöÄ Starting Comprehensive Wrong Detection Analysis")
        print("=" * 70)
        
        # Load dataset
        df = self.load_comprehensive_dataset()
        
        all_results = {}
        
        for interval in intervals:
            for similarity_method in similarity_methods:
                print(f"\n‚è∞ Analyzing {interval}min intervals with {similarity_method}...")
                
                # Load detection results
                results = self.load_detection_results(interval, similarity_method)
                
                if results is None:
                    continue
                
                # Analyze wrong detections
                analysis = self.analyze_wrong_detections(df, results, interval)
                
                # Store results
                key = f"{interval}min_{similarity_method}"
                all_results[key] = analysis
                
                # Generate report
                self.generate_wrong_detection_report(analysis, interval, similarity_method)
                
                # Create visualizations
                self.create_wrong_detection_visualizations(analysis)
        
        # Generate overall summary
        self.generate_overall_summary(all_results)
        
        return all_results
    
    def generate_overall_summary(self, all_results: dict):
        """Generate overall summary of wrong detection analysis"""
        print("\nüìä Generating Overall Wrong Detection Summary...")
        
        summary_data = []
        
        for key, results in all_results.items():
            interval, similarity_method = key.split('_', 1)
            
            summary_data.append({
                'Interval': interval,
                'Similarity_Method': similarity_method,
                'Precision': results['metrics']['precision'],
                'Recall': results['metrics']['recall'],
                'F1_Score': results['metrics']['f1_score'],
                'Accuracy': results['metrics']['accuracy'],
                'False_Positives': results['metrics']['false_positives'],
                'False_Negatives': results['metrics']['false_negatives'],
                'True_Positives': results['metrics']['true_positives'],
                'True_Negatives': results['metrics']['true_negatives'],
                'False_Positive_Rate': results['metrics']['false_positive_rate'],
                'False_Negative_Rate': results['metrics']['false_negative_rate']
            })
        
        summary_df = pd.DataFrame(summary_data)
        
        # Save overall summary
        summary_df.to_csv('wrong_detection_overall_summary.csv', index=False)
        
        # Print summary
        print("\nüìà Overall Wrong Detection Analysis Summary:")
        print("=" * 60)
        
        for _, row in summary_df.iterrows():
            print(f"\n‚è∞ {row['Interval']} | {row['Similarity_Method']}")
            print(f"   Precision: {row['Precision']:.3f} | Recall: {row['Recall']:.3f} | F1: {row['F1_Score']:.3f}")
            print(f"   False Positives: {row['False_Positives']} | False Negatives: {row['False_Negatives']}")
            print(f"   FP Rate: {row['False_Positive_Rate']:.3f} | FN Rate: {row['False_Negative_Rate']:.3f}")
        
        print(f"\n‚úÖ Overall summary saved to: wrong_detection_overall_summary.csv")

def main():
    """Main execution function"""
    print("üîç Wrong Detection Analysis for AgenticRAG")
    print("üìä CERT Insider Threat Dataset")
    print("=" * 70)
    
    # Initialize analysis framework
    analysis = WrongDetectionAnalysis()
    
    # Run comprehensive analysis
    results = analysis.run_comprehensive_analysis(
        intervals=[60, 120, 720],
        similarity_methods=['kl_divergence']
    )
    
    print("\nüéâ Wrong detection analysis completed successfully!")
    print("üìÅ Check the generated files for detailed analysis and recommendations.")

if __name__ == "__main__":
    main() 